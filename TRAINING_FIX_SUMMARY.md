# Training Fix Summary
============================================================

**"Talk is cheap. Show me the code."** - Linus Torvalds

## é—®é¢˜ (The Problem)

```
è®­ç»ƒå®ŒæˆåéªŒè¯å‡†ç¡®ç‡: 27.60%
è¿™TMåŸºæœ¬æ˜¯åƒåœ¾æ°´å¹³ (61ç±»åˆ†ç±», éšæœºçŒœæµ‹ = 1.6%)
```

## æ ¹æœ¬åŸå›  (Root Cause)

| é—®é¢˜ | å½±å“ | ä¸¥é‡æ€§ |
|------|------|--------|
| **Learning Rate = 1e-4** | å¤ªä¿å®ˆï¼Œå­¦ä¹ å¤ªæ…¢ | ğŸ”´ è‡´å‘½ |
| **æ—  Warmup** | æ—©æœŸç ´åé¢„è®­ç»ƒç‰¹å¾ | ğŸ”´ è‡´å‘½ |
| **Cosineä»Epoch 0å¼€å§‹** | ç«‹å³é™ä½LRï¼Œæ‰¼æ€å­¦ä¹  | ğŸ”´ è‡´å‘½ |
| **Batch Size = 64** | æœ‰æ•ˆLRå¤ªå° | ğŸŸ¡ ä¸¥é‡ |

## è§£å†³æ–¹æ¡ˆ (The Fix)

### 1. Learning Rate: 1e-4 â†’ 5e-4 (5x â†‘)

```python
# train.py line 128
default=5e-4  # ä» 1e-4 æå‡
```

### 2. Batch Size: 64 â†’ 32

```python
# train.py line 122
default=32  # ä» 64 é™ä½ï¼Œæ›´å¥½çš„æ¢¯åº¦ä¿¡å·
```

### 3. æ·»åŠ  Warmup (5 epochs)

```python
# train.py lines 402-419
# ä» 1e-5 çº¿æ€§å¢é•¿åˆ° 5e-4ï¼ŒæŒç»­5ä¸ªepoch
# ç„¶åå¼€å§‹ cosine decay
warmup_scheduler + cosine_scheduler â†’ SequentialLR
```

### 4. å®æ—¶å¯è§†åŒ– (NEW!)

```python
# trainer.py
# æ¯ä¸ªepochåè‡ªåŠ¨ç”Ÿæˆ training_curves.png
# åŒ…å«: Loss/Acc/LR/Overfittingåˆ†æ
```

## å¯¹æ¯” (Before vs After)

### è®­ç»ƒé…ç½®å¯¹æ¯”

| å‚æ•° | Before (åƒåœ¾) | After (æ­£ç¡®) | æå‡ |
|------|--------------|--------------|------|
| Learning Rate | 1e-4 | 5e-4 | **5x** |
| Batch Size | 64 | 32 | **2x** effective LR |
| Warmup | âŒ None | âœ… 5 epochs | **ç¨³å®šæ€§** |
| LR Schedule | ç«‹å³decay | Warmupådecay | **æ­£ç¡®æ—¶æœº** |
| **æ€»æœ‰æ•ˆLRæå‡** | - | - | **~10x** |

### LR Scheduleå¯¹æ¯”

**Before (é”™è¯¯):**
```
Epoch 0:  1e-4  â† ç«‹å³å¼€å§‹ï¼Œé©¬ä¸Šå°±decay
Epoch 10: 8e-5
Epoch 25: 5e-5
Epoch 50: 1e-6  â† å¤ªä½ï¼Œå‡ ä¹ä¸å­¦ä¹ 
```

**After (æ­£ç¡®):**
```
Epoch 0:  1e-5  â† å®‰å…¨èµ·æ­¥
Epoch 2:  2.5e-4 â† warmupä¸­
Epoch 5:  5e-4  â† è¾¾åˆ°å³°å€¼ï¼Œå¼€å§‹çœŸæ­£è®­ç»ƒ
Epoch 10: 4.8e-4 â† ç¼“æ…¢decay
Epoch 25: 2.5e-4 â† cosine decay
Epoch 50: 1e-6  â† æœ€ç»ˆæ”¶æ•›
```

### é¢„æœŸç»“æœ

| æŒ‡æ ‡ | Before | After (é¢„æœŸ) | æå‡ |
|------|--------|--------------|------|
| Val Accuracy | 27.6% | **70-85%** | **2.5-3x** |
| æ”¶æ•›é€Ÿåº¦ | æ…¢ | å¿« | **~5x** |
| ç¨³å®šæ€§ | å·® | å¥½ | âœ… |

## ä½¿ç”¨æ–¹æ³• (How to Use)

### æ–¹æ³•1: ä¸€é”®è®­ç»ƒ (æ¨è)

```bash
bash train_improved.sh
```

### æ–¹æ³•2: æ‰‹åŠ¨å‘½ä»¤

```bash
python train.py \
    --backbone resnet50 \
    --epochs 50 \
    --batch-size 32 \
    --lr 5e-4 \
    --optimizer adamw \
    --scheduler cosine \
    --save-dir checkpoints/task1_improved
```

### æŸ¥çœ‹è®­ç»ƒè¿›åº¦

```bash
# è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªepochåæŸ¥çœ‹:
open checkpoints/task1_improved/training_curves.png

# è®­ç»ƒååˆ†æ:
python visualize_training.py --checkpoint-dir checkpoints/task1_improved/
```

## å¯è§†åŒ–åŠŸèƒ½ (Visualization)

### è‡ªåŠ¨ç”Ÿæˆçš„å›¾è¡¨åŒ…å«:

1. **Loss Curves** - Train/Val losséšepochå˜åŒ–
2. **Accuracy Curves** - Train/Val accuracyï¼Œæ ‡æ³¨æœ€ä½³ç‚¹
3. **Learning Rate Schedule** - LRå˜åŒ–ï¼Œæ˜¾ç¤ºwarmupç»“æŸç‚¹
4. **Overfitting Analysis** - Train-Val gapåˆ†æ

### çŠ¶æ€åˆ¤æ–­:

| Train-Val Gap | çŠ¶æ€ | æ ‡è®° |
|---------------|------|------|
| < 5% | è‰¯å¥½æ‹Ÿåˆ | ğŸŸ¢ Green |
| 5-10% | è½»å¾®è¿‡æ‹Ÿåˆ | ğŸŸ¡ Orange |
| > 10% | è¿‡æ‹Ÿåˆ | ğŸ”´ Red |

## Linuså¼å“²å­¦ (Good Taste)

### âœ… æˆ‘ä»¬åšçš„ (Simple & Effective)

1. ä¿®å¤åŸºç¡€è¶…å‚æ•° (LR, batch size, warmup)
2. æ­£ç¡®çš„scheduleræ—¶æœº
3. ç®€å•ç›´æ¥çš„å¯è§†åŒ–

### âŒ æˆ‘ä»¬æ²¡åš (No Premature Optimization)

1. ~~å¤æ‚çš„optimizer~~ (AdamWå¤Ÿç”¨)
2. ~~èŠ±å“¨çš„augmentation~~ (å…ˆè®©åŸºç¡€è®­ç»ƒwork)
3. ~~æ¶æ„æœç´¢~~ (ResNet50å·²è¯æ˜æœ‰æ•ˆ)
4. ~~ensemble~~ (å•æ¨¡å‹éƒ½æ²¡trainå¥½)

> **"Premature optimization is the root of all evil."** - Knuth

å…ˆè®©åŸºç¡€çš„ä¸œè¥¿workï¼Œå†è€ƒè™‘ä¼˜åŒ–ã€‚

## å…³é”®æ–‡ä»¶ (Key Files)

### ä¿®æ”¹çš„æ–‡ä»¶:
- `train.py` - LR/batch size/warmup scheduler
- `trainer.py` - History tracking + plotting
- `config_task1.yaml` - é»˜è®¤è¶…å‚æ•°

### æ–°å¢çš„æ–‡ä»¶:
- `train_improved.sh` - ä¸€é”®è®­ç»ƒè„šæœ¬
- `visualize_training.py` - ç‹¬ç«‹å¯è§†åŒ–å·¥å…·
- `demo_visualization.py` - Demoæ¼”ç¤º
- `IMPROVEMENTS.md` - è¯¦ç»†æ–‡æ¡£
- `TRAINING_FIX_SUMMARY.md` - æœ¬æ–‡æ¡£

## å¿«é€Ÿè¯Šæ–­ (Quick Debug)

å¦‚æœå‡†ç¡®ç‡è¿˜æ˜¯ä½:

```bash
# 1. æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½
python -c "from dataset import *; ds = AgriDiseaseDataset('data/cleaned/train', 'data/cleaned/metadata/train_metadata.csv'); print(len(ds))"

# 2. æ£€æŸ¥class weightsæ˜¯å¦åŠ è½½
python -c "import pandas as pd; df = pd.read_csv('data/cleaned/metadata/class_weights.csv'); print(df.head())"

# 3. æŸ¥çœ‹è®­ç»ƒæ›²çº¿
python visualize_training.py --checkpoint-dir checkpoints/task1_improved/

# 4. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ”¹è¿›çš„è¶…å‚æ•°
grep -E "lr|batch" checkpoints/task1_improved/logs/*
```

## TL;DR (å¤ªé•¿ä¸çœ‹ç‰ˆ)

```
é—®é¢˜: 27.6% accuracy (åƒåœ¾)
åŸå› : LRå¤ªä½ + æ— warmup + scheduleré”™è¯¯
ä¿®å¤: LRâ†‘5x + batchâ†“2x + warmup + å¯è§†åŒ–
é¢„æœŸ: 70-85% accuracy
å‘½ä»¤: bash train_improved.sh
```

---

**æœ€åæ›´æ–°**: 2024-11-15  
**çŠ¶æ€**: âœ… Ready to use  
**æœŸæœ›æå‡**: 27.6% â†’ 70-85% (2.5-3x improvement)