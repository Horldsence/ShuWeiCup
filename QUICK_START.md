# Quick Start Guide - Fast Training
============================================================

**"Talk is cheap. Show me the code."** - Linus Torvalds

## ğŸš€ TL;DR - æœ€å¿«å¼€å§‹

```bash
# 1. åˆ›å»ºç²¾ç®€æ•°æ®é›† (å¦‚æœè¿˜æ²¡æœ‰)
python create_balanced_dataset.py

# 2. å¼€å§‹å¿«é€Ÿè®­ç»ƒ (3x faster!)
bash train_fast.sh

# 3. ç›‘æ§è®­ç»ƒè¿›åº¦
open checkpoints/task1_fast/training_curves.png
```

**é¢„æœŸæ—¶é—´**: ~16å°æ—¶ (vs å®Œæ•´æ•°æ®é›†48å°æ—¶)  
**é¢„æœŸå‡†ç¡®ç‡**: 60-70% (vs å®Œæ•´æ•°æ®é›†70-75%)

---

## ğŸ“‹ å®Œæ•´é—®é¢˜è¯Šæ–­ä¸è§£å†³æ–¹æ¡ˆ

### ğŸ”´ é—®é¢˜1: 6 Epochså°±æ”¶æ•›ï¼Œå‡†ç¡®ç‡åªæœ‰27-30%

**ä¸æ˜¯å­¦ä¹ ç‡é—®é¢˜ï¼** (ä½ è¯´å¯¹äº†)

**çœŸæ­£åŸå› **:
```
æç«¯ç±»åˆ«ä¸å¹³è¡¡:
  - Class 59: 2,445 samples
  - Class 44: 1 sample  
  - Class 45: 1 sample
  - æ¯”ä¾‹: 2445:1

åŸå§‹class weightså¤ªæ¿€è¿›:
  - èŒƒå›´: [0.211, 517.066]
  - æ¯”ä¾‹: 2445:1
  - ç»“æœ: rare classes dominate loss

æ¨¡å‹è¡Œä¸º:
  - Epoch 1-6: å¿«é€Ÿå­¦ä¹ major classes (9ç±» = 42%æ•°æ®)
  - è¾¾åˆ°30%å‡†ç¡®ç‡
  - Class weightså¯¼è‡´lossä¸ç¨³å®š
  - æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜: "çŒœå¤§ç±»"
```

### âœ… è§£å†³æ–¹æ¡ˆ (å·²å®æ–½)

#### 1. æ›´æ¸©å’Œçš„Class Weights (Critical!)
```python
# Before: 2445:1 ratio
weights = total / (n_classes * counts)  # [0.211, 517.066]

# After: 49:1 ratio  
weights = sqrt(max_count / counts)     # [0.226, 11.167]
```

#### 2. æ›´å¼ºçš„æ•°æ®å¢å¼º (Critical!)
```python
# Before (å¤ªå¼±)
A.Rotate(limit=15)
A.ColorJitter(..., p=0.5)

# After (aggressive)
A.Rotate(limit=45)           # å®Œæ•´æ—‹è½¬
A.ColorJitter(..., p=0.8)    # æ›´å¤šé¢œè‰²å˜åŒ–
A.VerticalFlip(p=0.3)        # å‚ç›´ç¿»è½¬
A.GridDistortion(p=0.3)      # ç½‘æ ¼æ‰­æ›²
A.RandomShadow(p=0.2)        # éšæœºé˜´å½±
```

#### 3. æ›´é«˜åˆ†è¾¨ç‡ (Important)
```
224x224 â†’ 320x320
```
å†œä½œç‰©ç—…å®³éœ€è¦ç»†èŠ‚ï¼Œ224å¤ªå°ã€‚

#### 4. ç²¾ç®€è®­ç»ƒé›† (Speed!)
```
31,541 samples â†’ 10,837 samples
æ¯ä¸ªç±»æœ€å¤š200å¼ 
è®­ç»ƒé€Ÿåº¦: 3x faster
```

---

## ğŸ¯ ä¸‰ç§è®­ç»ƒæ–¹æ¡ˆ

### æ–¹æ¡ˆA: å¿«é€Ÿè®­ç»ƒ (æ¨èç”¨äºå®éªŒ)

```bash
bash train_fast.sh
```

**é…ç½®**:
- æ•°æ®é›†: Balanced (10,837 samples)
- Class weights: Sqrt smoothing (49:1)
- åˆ†è¾¨ç‡: 320x320
- è®­ç»ƒæ—¶é—´: ~16å°æ—¶
- é¢„æœŸå‡†ç¡®ç‡: 60-70%

**é€‚ç”¨åœºæ™¯**:
- âœ… å¿«é€ŸåŸå‹éªŒè¯
- âœ… è¶…å‚æ•°æœç´¢
- âœ… æ¶æ„å®éªŒ
- âœ… æ—¶é—´<24å°æ—¶

---

### æ–¹æ¡ˆB: å®Œæ•´è®­ç»ƒ (æ¨èç”¨äºæœ€ç»ˆæ¨¡å‹)

```bash
bash train_fixed.sh
```

**é…ç½®**:
- æ•°æ®é›†: Full (31,541 samples)
- Class weights: Sqrt smoothing
- åˆ†è¾¨ç‡: 320x320
- ä¸¤é˜¶æ®µè®­ç»ƒ:
  - Stage 1: Head only (10 epochs)
  - Stage 2: Full fine-tune (40 epochs)
- è®­ç»ƒæ—¶é—´: ~48å°æ—¶
- é¢„æœŸå‡†ç¡®ç‡: 70-75%

**é€‚ç”¨åœºæ™¯**:
- âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒ
- âœ… ç«èµ›æäº¤
- âœ… ç”Ÿäº§éƒ¨ç½²
- âœ… è¿½æ±‚æœ€é«˜å‡†ç¡®ç‡

---

### æ–¹æ¡ˆC: è¶…å¿«è®­ç»ƒ (ä»…ç”¨äºdebug)

```bash
# è‡ªå®šä¹‰: æ›´å°‘epochs
python train.py \
    --train-meta data/cleaned/metadata/train_metadata_balanced.csv \
    --class-weights data/cleaned/metadata/class_weights_sqrt.csv \
    --epochs 10 \
    --batch-size 32 \
    --lr 5e-4 \
    --image-size 224 \
    --save-dir checkpoints/debug
```

**é…ç½®**:
- æ•°æ®é›†: Balanced
- åˆ†è¾¨ç‡: 224x224 (æ›´å¿«)
- Epochs: 10 (å¿«é€ŸéªŒè¯)
- è®­ç»ƒæ—¶é—´: ~2å°æ—¶
- é¢„æœŸå‡†ç¡®ç‡: 40-50%

**é€‚ç”¨åœºæ™¯**:
- âœ… ä»£ç è°ƒè¯•
- âœ… å¿«é€ŸéªŒè¯ä¿®æ”¹
- âœ… CI/CDæµ‹è¯•

---

## ğŸ“Š å®æ—¶ç›‘æ§

### è®­ç»ƒè¿‡ç¨‹ä¸­
```bash
# æŸ¥çœ‹å®æ—¶æ›²çº¿ (æ¯ä¸ªepochè‡ªåŠ¨æ›´æ–°)
open checkpoints/task1_fast/training_curves.png

# æŸ¥çœ‹æ§åˆ¶å°è¾“å‡º
# - Epochè¿›åº¦
# - Train/Val loss & accuracy
# - Learning rateå˜åŒ–
```

### å›¾è¡¨åŒ…å«
- **Loss Curves**: Train/Val losså˜åŒ–
- **Accuracy Curves**: Train/Val accuracy + æœ€ä½³ç‚¹æ ‡è®°
- **LR Schedule**: å­¦ä¹ ç‡å˜åŒ–ï¼Œæ˜¾ç¤ºwarmupé˜¶æ®µ
- **Overfitting Analysis**: Train-Val gapåˆ†æ
  - ğŸŸ¢ Green: Good fit (<5% gap)
  - ğŸŸ¡ Orange: Slight overfitting (5-10%)
  - ğŸ”´ Red: Overfitting (>10%)

### è®­ç»ƒååˆ†æ
```bash
# è¯¦ç»†å¯è§†åŒ–
python visualize_training.py --checkpoint-dir checkpoints/task1_fast/

# å¯¹æ¯”ä¸åŒè¿è¡Œ
python visualize_training.py --compare \
    checkpoints/task1_fast/best.pth \
    checkpoints/task1_stage2/best.pth

# æ•°æ®é›†å¯¹æ¯”
python compare_datasets.py --visualize
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| é…ç½® | æ•°æ®é›† | è®­ç»ƒæ—¶é—´ | é¢„æœŸå‡†ç¡®ç‡ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|------------|----------|
| **å¿«é€Ÿè®­ç»ƒ** | 10.8k | 16h | 60-70% | å®éªŒ/åŸå‹ |
| **å®Œæ•´è®­ç»ƒ** | 31.5k | 48h | 70-75% | æœ€ç»ˆæ¨¡å‹ |
| **è¶…å¿«Debug** | 10.8k | 2h | 40-50% | ä»£ç éªŒè¯ |
| **åŸå§‹(é”™è¯¯)** | 31.5k | 100h | ~30% | âŒ ä¸æ¨è |

---

## ğŸ” æ•…éšœæ’æŸ¥

### å‡†ç¡®ç‡è¿˜æ˜¯å¾ˆä½ (<40%)

**æ£€æŸ¥æ¸…å•**:

```bash
# 1. éªŒè¯class weightsæ˜¯å¦ä½¿ç”¨
grep "class_weights_sqrt" train_fast.sh
python -c "import pandas as pd; df = pd.read_csv('data/cleaned/metadata/class_weights_sqrt.csv'); print(df.head())"

# 2. éªŒè¯æ•°æ®å¢å¼ºæ˜¯å¦åº”ç”¨
grep "rotate_limit=45" dataset.py

# 3. æŸ¥çœ‹è®­ç»ƒæ›²çº¿
open checkpoints/task1_fast/training_curves.png
# Lossåº”è¯¥åœ¨ä¸‹é™ï¼Œä¸æ˜¯å¹³çš„

# 4. æ£€æŸ¥æ•°æ®åŠ è½½
python -c "from dataset import *; ds = AgriDiseaseDataset('data/cleaned/train', 'data/cleaned/metadata/train_metadata_balanced.csv'); print(f'Samples: {len(ds)}')"
```

### è®­ç»ƒå¤ªæ…¢

```bash
# å‡å°batch size (å¦‚æœGPUå†…å­˜ä¸è¶³)
python train.py --batch-size 16 ...

# é™ä½åˆ†è¾¨ç‡
python train.py --image-size 224 ...

# å‡å°‘workers
python train.py --num-workers 2 ...

# ä½¿ç”¨æ›´å°çš„backbone
python train.py --backbone resnet34 ...
```

### æ˜¾å­˜ä¸è¶³

```bash
# 1. å‡å°batch size
--batch-size 16  # or 8

# 2. é™ä½åˆ†è¾¨ç‡
--image-size 224  # or 192

# 3. ç¦ç”¨AMP (å¦‚æœæœ‰é—®é¢˜)
# ç§»é™¤ --use-amp å‚æ•°

# 4. å‡å°‘workers
--num-workers 2
```

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

- **[REAL_FIX.md](REAL_FIX.md)** - è¯¦ç»†é—®é¢˜è¯Šæ–­å’Œä¿®å¤æ–¹æ¡ˆ
- **[TRAINING_FIX_SUMMARY.md](TRAINING_FIX_SUMMARY.md)** - å­¦ä¹ ç‡ä¿®å¤æ€»ç»“ (æ¬¡è¦)
- **[IMPROVEMENTS.md](IMPROVEMENTS.md)** - å®Œæ•´æ”¹è¿›æ–‡æ¡£
- **[README.md](README.md)** - é¡¹ç›®ä¸»æ–‡æ¡£

---

## ğŸ“ å…³é”®æ•™è®­ (Linus-Style)

### 1. æ•°æ® > è¶…å‚æ•°

```
"Bad programmers worry about code.
 Good programmers worry about data." - Linus
```

é—®é¢˜ä¸åœ¨äº:
- âŒ Learning rate
- âŒ Optimizer
- âŒ Model architecture

é—®é¢˜åœ¨äº:
- âœ… æç«¯ç±»åˆ«ä¸å¹³è¡¡ (2445:1)
- âœ… é”™è¯¯çš„weightingç­–ç•¥
- âœ… æ•°æ®å¢å¼ºä¸è¶³

### 2. å¬ç”¨æˆ·çš„ (ä½ è¯´å¯¹äº†)

```
"6 epochså°±ç¨³å®šï¼Œå‡†ç¡®ç‡è¿‡ä½ï¼Œè¿™ä¸æ˜¯å­¦ä¹ ç‡å½±å“çš„"
```

è¿™å¥è¯æ˜¯å…³é”®ã€‚å¿«é€Ÿæ”¶æ•›+ä½å‡†ç¡®ç‡ = è§£å†³äº†**é”™è¯¯çš„é—®é¢˜**ã€‚

æ¨¡å‹åœ¨é«˜æ•ˆå­¦ä¹ ï¼Œåªæ˜¯å­¦ä¹ äº†:
- "é¢„æµ‹å¤§ç±»" â†’ 30% å‡†ç¡®ç‡ï¼Œ6 epochsæå®š
- ä»»åŠ¡å®Œæˆ (ä»æ¨¡å‹è§’åº¦)

### 3. ä¿®å¤æ ¹æœ¬åŸå› 

```python
# é”™è¯¯åšæ³• (æ²»æ ‡ä¸æ²»æœ¬)
lr = 5e-4  # æé«˜å­¦ä¹ ç‡
# â†’ æ¨¡å‹ä¾ç„¶30%æ”¶æ•›

# æ­£ç¡®åšæ³• (ä¿®å¤æ ¹å› )  
class_weights = sqrt_smoothing()  # å¹³è¡¡æ•°æ®
augmentation = strong()           # ç”Ÿæˆæ›´å¤šæ ·æœ¬
# â†’ æ¨¡å‹æ­£å¸¸å­¦ä¹ 
```

### 4. è¿‡æ—©ä¼˜åŒ–æ˜¯ä¸‡æ¶ä¹‹æº

```
åˆå§‹ä¿®å¤:
âœ“ LRè°ƒä¼˜ (å°å¹…æ”¹è¿›)
âœ“ Warmup (ç¨³å®šæ€§)
âœ“ Scheduler (å¹³æ»‘æ€§)

çœŸæ­£é—®é¢˜:
âœ“ æ•°æ®ä¸å¹³è¡¡ (2-3xæ”¹è¿›)
```

æˆ‘ä»¬èŠ±æ—¶é—´ä¼˜åŒ–è¶…å‚æ•°ï¼Œä½†çœŸæ­£é—®é¢˜æ˜¯æ•°æ®åˆ†å¸ƒã€‚

**æ•™è®­: æ°¸è¿œå…ˆæ£€æŸ¥æ•°æ®ã€‚**

---

## ğŸš€ æ¨èå·¥ä½œæµ

### Phase 1: å¿«é€Ÿå®éªŒ (1-2å¤©)
```bash
# ä½¿ç”¨balanced dataset
bash train_fast.sh

# å°è¯•ä¸åŒé…ç½®:
# - ä¸åŒçš„dropout (0.3, 0.4, 0.5)
# - ä¸åŒçš„augmentationå¼ºåº¦
# - ä¸åŒçš„backbone (resnet34, resnet50, efficientnet)

# æ‰¾åˆ°æœ€ä½³é…ç½®
```

### Phase 2: å®Œæ•´è®­ç»ƒ (2-3å¤©)
```bash
# ä½¿ç”¨full dataset + æœ€ä½³é…ç½®
bash train_fixed.sh

# é¢„æœŸ: æ¯”balancedé«˜2-3%å‡†ç¡®ç‡
```

### Phase 3: åˆ†æä¼˜åŒ–
```bash
# åˆ†æper-classæ€§èƒ½
python analyze_data.py --visualize

# å¯¹æ¯”ä¸åŒæ¨¡å‹
python visualize_training.py --compare \
    checkpoints/*/best.pth

# é”™è¯¯æ¡ˆä¾‹åˆ†æ
# (éœ€è¦å•ç‹¬å®ç°)
```

---

## âœ… æœ€ç»ˆæ£€æŸ¥æ¸…å•

è®­ç»ƒå‰ç¡®è®¤:

- [ ] Balanced datasetå·²åˆ›å»º
  ```bash
  ls data/cleaned/metadata/train_metadata_balanced.csv
  ```

- [ ] Sqrt class weightså·²ç”Ÿæˆ
  ```bash
  ls data/cleaned/metadata/class_weights_sqrt.csv
  ```

- [ ] æ•°æ®å¢å¼ºå·²æ›´æ–°
  ```bash
  grep "rotate_limit=45" dataset.py
  ```

- [ ] è®­ç»ƒè„šæœ¬å¯æ‰§è¡Œ
  ```bash
  ls -lh train_fast.sh
  ```

- [ ] GPUå¯ç”¨ (æ¨è)
  ```bash
  python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
  ```

---

## ğŸ¯ æœŸæœ›ç»“æœ

### Balanced Dataset (Fast)
```
è®­ç»ƒæ—¶é—´: ~16å°æ—¶
Epoch 10: ~45-50% val accuracy
Epoch 30: ~60-65% val accuracy  
Epoch 50: ~65-70% val accuracy
```

### Full Dataset (Complete)
```
è®­ç»ƒæ—¶é—´: ~48å°æ—¶
Stage 1 (10 epochs): ~40-45% val accuracy
Stage 2 (40 epochs): ~70-75% val accuracy
```

### å¦‚æœå‡†ç¡®ç‡ä½äºé¢„æœŸ
1. æ£€æŸ¥class weightsæ˜¯å¦ä½¿ç”¨
2. æŸ¥çœ‹training curvesæ˜¯å¦æ­£å¸¸
3. éªŒè¯æ•°æ®augmentation
4. è€ƒè™‘è°ƒæ•´hyperparameters

---

**ç°åœ¨å¼€å§‹**: `bash train_fast.sh` ğŸš€

**é¢„æœŸæ—¶é—´**: 16å°æ—¶  
**é¢„æœŸå‡†ç¡®ç‡**: 65-70%  
**åŠ é€Ÿæ¯”**: 3x faster than full dataset